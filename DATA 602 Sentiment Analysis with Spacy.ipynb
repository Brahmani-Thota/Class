{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When learning data science, you shouldn't get discouraged!\", \"\\nChallenges and setbacks aren't failures, they're just part of the journey.\", \"You've got this!\"]\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# create list of sentence tokens\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['any', 'regarding', 'make', '’re', 'first', 'either', 'seemed', 'another', 'but', 'say', \"'s\", 'hereby', 'formerly', 'over', 'anyhow', 'can', 'toward', 'yours', 'still', 'thereby']\n"
     ]
    }
   ],
   "source": [
    "#Stop words\n",
    "#importing stop words from English language.\n",
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: [learning, data, science, ,, discouraged, !, \n",
      ", Challenges, setbacks, failures, ,, journey, ., got, !]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#Implementation of stop words:\n",
    "filtered_sent=[]\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# filtering stop words\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "runs runs\n",
      "running running\n",
      "runner runner\n"
     ]
    }
   ],
   "source": [
    "# Implementing lemmatization\n",
    "lem = nlp(\"run runs running runner\")\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DET\n",
      "is AUX\n",
      "well ADJ\n",
      "that DET\n",
      "ends VERB\n",
      "well ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "\n",
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "#  \"nlp\" Objectis used to create documents with linguistic annotations.\n",
    "docs = nlp(u\"All is well that ends well.\")\n",
    "\n",
    "for word in docs:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(New York City, 'GPE', 384),\n",
       " (Tuesday, 'DATE', 391),\n",
       " (At least 285, 'CARDINAL', 397),\n",
       " (September, 'DATE', 391),\n",
       " (Brooklyn, 'GPE', 384),\n",
       " (Williamsburg, 'GPE', 384),\n",
       " (four, 'CARDINAL', 397),\n",
       " (Bill de Blasio, 'PERSON', 380),\n",
       " (Tuesday, 'DATE', 391),\n",
       " (Orthodox Jews, 'ORG', 383),\n",
       " (6 months old, 'DATE', 391),\n",
       " (up to $1,000, 'MONEY', 394)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for visualization of Entity detection importing displacy from spacy:\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "nytimes= nlp(u\"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
    "At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")\n",
    "\n",
    "entities=[(i, i.label_, i.label) for i in nytimes.ents]\n",
    "entities    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pursuit pursuit pobj In\n",
      "a wall wall pobj of\n",
      "President Trump Trump nsubj ran\n"
     ]
    }
   ],
   "source": [
    "docp = nlp (\" In pursuit of a wall, President Trump ran into one.\")\n",
    "\n",
    "for chunk in docp.noun_chunks:\n",
    "   print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n",
      "[ 0.20538223 -1.6033714   0.27122334  0.4102599   3.2985601   3.4889512\n",
      "  1.8090308  -2.1398475   2.31565     1.5809067   4.1519527  -1.0185633\n",
      " -0.0325011  -2.7471437  -0.4177467  -2.4292274  -0.6153387   2.4422317\n",
      "  0.8078671  -2.4846377   2.0988142   1.4448209  -0.552992   -1.3411183\n",
      " -0.69847786 -0.45548356  3.8267968  -4.0225782   0.81215733  0.3766132\n",
      "  0.15751392 -1.1428392  -1.3328214   0.7187766   2.1567593  -3.018766\n",
      "  3.4919028   0.6938907  -1.1943094  -0.10796624  4.7029977   3.551554\n",
      " -0.71505725 -4.4580555  -0.26480573  0.6314918  -0.538128   -1.1131921\n",
      " -1.1251849   0.5740081  -1.1976193  -3.5157654   0.425157   -1.7545594\n",
      " -3.058784    0.01680815  0.97784567  1.7633746   0.4561966   2.5090182\n",
      "  0.35267782  0.8351371  -1.394351    0.5082075   0.75960976 -3.3654122\n",
      "  2.3440146  -2.4311178   1.2401564  -1.4498216  -2.3708577   1.274456\n",
      "  2.6584334   2.505236    0.24999112  0.45838034  0.7396465  -3.0134087\n",
      " -1.1449497   2.441533    0.58746856 -0.47240722 -0.99527466 -2.6816308\n",
      "  0.09063269  6.415723   -2.0405467   0.45456767 -1.6633052  -0.92034495\n",
      "  0.12515879 -1.3880293  -2.1336856   2.3337066  -0.68828297 -1.3300103 ]\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "mango = nlp(u'mango')\n",
    "print(mango.vector.shape)\n",
    "print(mango.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TSV file\n",
    "df_amazon = pd.read_csv (\"d:/Machine Learning/amazon_alexa 2.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date         variation  \\\n",
       "0       5  31-Jul-18  Charcoal Fabric    \n",
       "1       5  31-Jul-18  Charcoal Fabric    \n",
       "2       4  31-Jul-18    Walnut Finish    \n",
       "3       5  31-Jul-18  Charcoal Fabric    \n",
       "4       5  31-Jul-18  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  \n",
       "0                                      Love my Echo!         1  \n",
       "1                                          Loved it!         1  \n",
       "2  Sometimes while playing a game, you can answer...         1  \n",
       "3  I have had a lot of fun with this thing. My 4 ...         1  \n",
       "4                                              Music         1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 records\n",
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataframe\n",
    "df_amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      "rating              3150 non-null int64\n",
      "date                3150 non-null object\n",
      "variation           3150 non-null object\n",
      "verified_reviews    3150 non-null object\n",
      "feedback            3150 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 123.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# View data information\n",
    "df_amazon.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2893\n",
       "0     257\n",
       "Name: feedback, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedback Value count\n",
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>3150.000000</td>\n",
       "      <td>3150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4.463175</td>\n",
       "      <td>0.918413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.068506</td>\n",
       "      <td>0.273778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rating     feedback\n",
       "count  3150.000000  3150.000000\n",
       "mean      4.463175     0.918413\n",
       "std       1.068506     0.273778\n",
       "min       1.000000     0.000000\n",
       "25%       4.000000     1.000000\n",
       "50%       5.000000     1.000000\n",
       "75%       5.000000     1.000000\n",
       "max       5.000000     1.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()  # use directly\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tony Diana\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x000001C324F29A08>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x000001C31AF9C048>,\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9333333333333333\n",
      "Logistic Regression Precision: 0.9397590361445783\n",
      "Logistic Regression Recall: 0.9907621247113164\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24,  55],\n",
       "       [  8, 858]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predicted)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Table               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.99      0.95       851\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.92      0.99      0.95       851\n",
      "   macro avg       0.18      0.20      0.19       851\n",
      "weighted avg       0.92      0.99      0.95       851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['1 rating', '2 rating', '3 rating', '4 rating', '5 rating']\n",
    "print(\"Classification Table\", classification_report(y_test, predicted, labels=[1, 2, 3, 4, 5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 9662/1000\n",
      "train shape: (9662, 56)\n",
      "dev shape: (1000, 56)\n",
      "vocab_size 18766\n",
      "sentence max words 56\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen\n",
    "    \n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def download_sentences(url):\n",
    "    \"\"\"\n",
    "    Download sentences from specified URL. \n",
    "    \n",
    "    Strip trailing newline, convert to Unicode.\n",
    "    \"\"\"\n",
    "    \n",
    "    remote_file = urlopen(url)\n",
    "    return [line.decode('Latin1').strip() for line in remote_file.readlines()]\n",
    "    \n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    positive_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos')\n",
    "    negative_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg')\n",
    "    \n",
    "    # Tokenize\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent).split(\" \") for sent in x_text]\n",
    "\n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [0 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to be the length of the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "        \n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from token to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    \n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    \n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    \n",
    "    return vocabulary, vocabulary_inv\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([\n",
    "            [vocabulary[word] for word in sentence]\n",
    "            for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\"\"\"\n",
    "Loads and preprocesses data for the MR dataset.\n",
    "Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "\"\"\"\n",
    "# Load and preprocess data\n",
    "sentences, labels = load_data_and_labels()\n",
    "sentences_padded = pad_sentences(sentences)\n",
    "vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# split train/dev set\n",
    "# there are a total of 10662 labeled examples to train on\n",
    "x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]\n",
    "\n",
    "sentence_size = x_train.shape[1]\n",
    "\n",
    "print('Train/Dev split: %d/%d' % (len(y_train), len(y_dev)))\n",
    "print('train shape:', x_train.shape)\n",
    "print('dev shape:', x_dev.shape)\n",
    "print('vocab_size', vocab_size)\n",
    "print('sentence max words', sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 50\n",
      "embedding dimensions 300\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import sys,os\n",
    "\n",
    "'''\n",
    "Define batch size and the place holders for network inputs and outputs\n",
    "'''\n",
    "\n",
    "batch_size = 50\n",
    "print('batch size', batch_size)\n",
    "\n",
    "input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "\n",
    "\n",
    "'''\n",
    "Define the first network layer (embedding)\n",
    "'''\n",
    "\n",
    "# create embedding layer to learn representation of words in a lower dimensional subspace (much like word2vec)\n",
    "num_embed = 300 # dimensions to embed words into\n",
    "print('embedding dimensions', num_embed)\n",
    "\n",
    "embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n",
    "\n",
    "# reshape embedded data for next layer\n",
    "conv_input = mx.sym.Reshape(data=embed_layer, shape=(batch_size, 1, sentence_size, num_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolution filters [3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# create convolution + (max) pooling layer for each filter operation\n",
    "filter_list=[3, 4, 5] # the size of filters to use\n",
    "print('convolution filters', filter_list)\n",
    "\n",
    "num_filter=100\n",
    "pooled_outputs = []\n",
    "for filter_size in filter_list:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = num_filter * len(filter_list)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout probability 0.5\n"
     ]
    }
   ],
   "source": [
    "# dropout layer\n",
    "dropout = 0.5\n",
    "print('dropout probability', dropout)\n",
    "\n",
    "if dropout > 0.0:\n",
    "    h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "else:\n",
    "    h_drop = h_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected layer\n",
    "num_label = 2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_drop, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Define the structure of our CNN Model (as a named tuple)\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# Define what device to train/test on, use GPU if available\n",
    "ctx = mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "\n",
    "arg_names = cnn.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batch_size, sentence_size)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer rmsprop\n",
      "maximum gradient 5.0\n",
      "learning rate (step size) 0.0005\n",
      "epochs to train for 50\n",
      "Iter [0] Train: Time: 56.420s, Training Accuracy: 56.000             --- Dev Accuracy thus far: 64.000\n",
      "Iter [1] Train: Time: 61.748s, Training Accuracy: 72.591             --- Dev Accuracy thus far: 69.500\n",
      "Iter [2] Train: Time: 65.649s, Training Accuracy: 81.140             --- Dev Accuracy thus far: 73.000\n",
      "Iter [3] Train: Time: 62.162s, Training Accuracy: 87.067             --- Dev Accuracy thus far: 75.400\n",
      "Iter [4] Train: Time: 62.869s, Training Accuracy: 90.943             --- Dev Accuracy thus far: 76.600\n",
      "Iter [5] Train: Time: 59.268s, Training Accuracy: 93.751             --- Dev Accuracy thus far: 77.300\n",
      "Iter [6] Train: Time: 59.234s, Training Accuracy: 96.238             --- Dev Accuracy thus far: 77.600\n",
      "Iter [7] Train: Time: 65.138s, Training Accuracy: 97.585             --- Dev Accuracy thus far: 77.700\n",
      "Iter [8] Train: Time: 58.988s, Training Accuracy: 98.373             --- Dev Accuracy thus far: 77.300\n",
      "Saved checkpoint to ./cnn-0009.params\n",
      "Iter [9] Train: Time: 58.404s, Training Accuracy: 99.057             --- Dev Accuracy thus far: 77.700\n",
      "Iter [10] Train: Time: 58.789s, Training Accuracy: 99.389             --- Dev Accuracy thus far: 77.600\n",
      "Iter [11] Train: Time: 58.840s, Training Accuracy: 99.668             --- Dev Accuracy thus far: 77.800\n",
      "Iter [12] Train: Time: 59.114s, Training Accuracy: 99.813             --- Dev Accuracy thus far: 76.800\n",
      "Iter [13] Train: Time: 62.434s, Training Accuracy: 99.855             --- Dev Accuracy thus far: 77.800\n",
      "Iter [14] Train: Time: 59.689s, Training Accuracy: 99.948             --- Dev Accuracy thus far: 77.600\n",
      "Iter [15] Train: Time: 58.871s, Training Accuracy: 99.979             --- Dev Accuracy thus far: 76.800\n",
      "Iter [16] Train: Time: 62.876s, Training Accuracy: 99.938             --- Dev Accuracy thus far: 76.500\n",
      "Iter [17] Train: Time: 60.566s, Training Accuracy: 99.990             --- Dev Accuracy thus far: 77.100\n",
      "Iter [18] Train: Time: 61.227s, Training Accuracy: 99.979             --- Dev Accuracy thus far: 77.200\n",
      "Saved checkpoint to ./cnn-0019.params\n",
      "Iter [19] Train: Time: 59.908s, Training Accuracy: 99.990             --- Dev Accuracy thus far: 77.300\n",
      "Iter [20] Train: Time: 59.820s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 76.700\n",
      "Iter [21] Train: Time: 59.566s, Training Accuracy: 99.979             --- Dev Accuracy thus far: 77.000\n",
      "Iter [22] Train: Time: 63.707s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 76.900\n",
      "Iter [23] Train: Time: 62.399s, Training Accuracy: 99.990             --- Dev Accuracy thus far: 77.500\n",
      "Iter [24] Train: Time: 59.421s, Training Accuracy: 99.979             --- Dev Accuracy thus far: 76.800\n",
      "Iter [25] Train: Time: 62.882s, Training Accuracy: 99.979             --- Dev Accuracy thus far: 76.300\n",
      "Iter [26] Train: Time: 60.227s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 76.900\n",
      "Iter [27] Train: Time: 61.191s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 76.500\n",
      "Iter [28] Train: Time: 62.774s, Training Accuracy: 99.990             --- Dev Accuracy thus far: 77.600\n",
      "Saved checkpoint to ./cnn-0029.params\n",
      "Iter [29] Train: Time: 60.620s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 77.500\n",
      "Iter [30] Train: Time: 61.139s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 77.900\n",
      "Iter [31] Train: Time: 63.182s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 76.600\n",
      "Iter [32] Train: Time: 63.675s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 77.700\n",
      "Iter [33] Train: Time: 63.609s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 77.500\n",
      "Iter [34] Train: Time: 62.351s, Training Accuracy: 100.000             --- Dev Accuracy thus far: 77.100\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the cnn_model using back prop\n",
    "'''\n",
    "\n",
    "optimizer = 'rmsprop'\n",
    "max_grad_norm = 5.0\n",
    "learning_rate = 0.0005\n",
    "epoch = 50\n",
    "\n",
    "print('optimizer', optimizer)\n",
    "print('maximum gradient', max_grad_norm)\n",
    "print('learning rate (step size)', learning_rate)\n",
    "print('epochs to train for', epoch)\n",
    "\n",
    "# create optimizer\n",
    "opt = mx.optimizer.create(optimizer)\n",
    "opt.lr = learning_rate\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "# For each training epoch\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Over each batch of training data\n",
    "    for begin in range(0, x_train.shape[0], batch_size):\n",
    "        batchX = x_train[begin:begin+batch_size]\n",
    "        batchY = y_train[begin:begin+batch_size]\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        cnn_model.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        # backward\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "\n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batch_size\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # Decay learning rate for this epoch to ensure we are not \"overshooting\" optima\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "\n",
    "    # End of training loop for this epoch\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    # Saving checkpoint to disk\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "        print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "    # Evaluate model after this epoch on dev (test) set\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # For each test batch\n",
    "    for begin in range(0, x_dev.shape[0], batch_size):\n",
    "        batchX = x_dev[begin:begin+batch_size]\n",
    "        batchY = y_dev[begin:begin+batch_size]\n",
    "\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "\n",
    "    dev_acc = num_correct * 100 / float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n",
    "            --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
